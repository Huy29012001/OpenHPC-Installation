# vi /etc/slurm/slurm.conf

https://slurm.schedmd.com/configurator.html

#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=galaxy.cluster
SlurmctldHost=master
#ControlMachine=master
#ControlAddr=192.168.1.254
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/spool/slurm/ctld
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
TaskPlugin=task/none
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SelectType=select/linear
FastSchedule=1
SchedulerPort=7321
SelectType=select/cons_res
SelectTypeParameters=CR_CPU
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/none
#JobAcctGatherFrequency=30
#
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=master
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStorageUser=
#
# COMPUTE NODES
# OpenHPC default configuration
#PropagateResourceLimitsExcept=MEMLOCK
#AccountingStorageType=accounting_storage/filetxt
#Epilog=/etc/slurm/slurm.epilog.clean
#
GresTypes=gpu
NodeName=c1 NodeAddr=c1 NodeHostName=c1 Gres=gpu:1 Procs=12 Sockets=1 CoresPerSocket=6 ThreadsPerCore=2 State=UNKNOWN RealMemory=55000
NodeName=c2 NodeAddr=c2 NodeHostName=c2 Gres=gpu:1 Procs=12 Sockets=1 CoresPerSocket=6 ThreadsPerCore=2 State=UNKNOWN RealMemory=25000
NodeName=master NodeAddr=master NodeHostName=master Gres=gpu:2 Procs=8 Sockets=1 CoresPerSocket=4 ThreadsPerCore=2 State=UNKNOWN RealMemory=55000
PartitionName=qcpu Nodes=c[1-2],master Shared=NO Default=YES State=UP
PartitionName=qgpu Nodes=master,c[1-2] Shared=NO Default=NO MaxCPUsPerNode=2 State=UP
DebugFlags=NO_CONF_HASH
ReturnToService=2


Creat file gres.conf

# vi /etc/slurm/gres.conf
NodeName=master Name=gpu File=/dev/nvidia[0-1]
NodeName=c1 Name=gpu File=/dev/nvidia0
NodeName=c2 Name=gpu File=/dev/nvidia0

systemctl restart slurmctld
systemctl restart slurmd
systemctl restart munge
pdsh -w c[1-2] systemctl restart slurmd
pdsh -w c[1-2] systemctl restart munge
scontrol show nodes

Change state of node

# scontrol for compute node after reboot
scontrol update NodeName=c[1-2] State=RESUME

Setup Slurm Database

# cp /etc/slurm/slurmdbd.conf.example /etc/slurm/slurmdbd.conf

#DbdAddr=localhost
DbdHost=master
# Database info
StorageType=accounting_storage/mysql
StorageHost=localhost
#StoragePort=1234
StoragePass=slurm1234
StorageUser=slurm
StorageLoc=slurm_acct_db

mkdir /var/log/slurm
touch /var/log/slurm/slurmdbd.log

